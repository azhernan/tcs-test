# TCS Demo ‚Äî FastAPI + PostgreSQL + Airflow + RabbitMQ/Celery

> Proyecto de ejemplo listo simple. Incluye API (FastAPI), base de datos (PostgreSQL), orquestaci√≥n (Airflow), mensajer√≠a (RabbitMQ) y tareas as√≠ncronas (Celery), con persistencia en Docker y hooks de calidad.

## Arquitectura

- **FastAPI**: API HTTP (`/health`, `/items`, `/items/{id}/process`, `/items/{name}/history`, `/items/latest`).
- **PostgreSQL**: `items` (idempotente por `name`) + `item_prices` (hist√≥rico).
- **Airflow**: DAG `daily_prices_etl` (UPSERT + hist√≥rico) y `cleanup_item_prices` (retenci√≥n).
- **RabbitMQ + Celery**: tareas as√≠ncronas (procesar item).

## Levantar servicioes

```bash
docker compose up -d db
docker compose up -d --build api rabbitmq worker
docker compose up -d airflow-init
docker compose up -d airflow-webserver airflow-scheduler

---

## ‚ú® Qu√© hace
- **API (FastAPI)** expone endpoints REST:
  - `POST /items` ‚Äî crear items.
  - `POST /items/{id}/process` ‚Äî encola una tarea Celery para procesar el item.
  - `GET /items/{name}/history` ‚Äî hist√≥rico de precios (append-only).
  - `GET /items/latest` ‚Äî √∫ltimo precio por item.
- **Airflow**
  - DAG `daily_prices_etl`: genera CSV, transforma con pandas y **UPSERT** en `items`; registra hist√≥rico en `item_prices` (idempotente por `name`).
  - DAG `cleanup_item_prices`: retenci√≥n de hist√≥rico (por ejemplo, 30 d√≠as).
  - (Opcional) DAG `backup_pg`: backup diario con `pg_dump`.
- **Celery + RabbitMQ**: procesamiento as√≠ncrono de trabajos publicados por la API.
- **PostgreSQL**: base `tcsdb` con tablas `items` e `item_prices` (hist√≥rico).
- **Persistencia**: volumen Docker `pgdata` montado en `/var/lib/postgresql/data`.

---

## üß© Arquitectura (resumen)

```

[ FastAPI ]  --HTTP-->  (create/process/history)        ‚îê
     ‚îÇ                               ‚îÇ                  ‚îÇ
     ‚îÇ                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
     ‚îî‚îÄ publish task  ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   RabbitMQ (AMQP) ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
                                     ‚îÇ                  ‚îÇ
                               [ Celery Worker ]  ------‚îò
                                     ‚îÇ
                                     ‚ñº
                                [ PostgreSQL ]
                           items / item_prices

[ Airflow ]: DAGs diarios (ETL / retenci√≥n / backup) leyendo/escribiendo en PostgreSQL.

```

---

## üöÄ Levantar el entorno (Docker Compose)

> Requisitos: Docker Desktop (WSL2 en Windows recomendado) y `docker compose`.

```bash

# 1) Base de datos (con volumen persistente)
docker compose up -d db

# 2) API, cola y worker
docker compose up -d --build api rabbitmq worker

# 3) Inicializar Airflow (primera vez)
docker compose up -d airflow-init

# 4) Webserver y Scheduler de Airflow
docker compose up -d airflow-webserver airflow-scheduler
```

### URLs √∫tiles

- **API**: <http://localhost:8000>
- **OpenAPI/Docs**: <http://localhost:8000/docs>
- **Airflow**: <http://localhost:8080>  (usuario/clave por defecto: `admin`/`admin`)
- **RabbitMQ**: <http://localhost:15672>  (`guest`/`guest`)
- **PostgreSQL**: `localhost:5432` (`tcs`/`tcs`, DB `tcsdb`)

---

## üóÇÔ∏è Servicios (compose)

- **db** (PostgreSQL 16): volumen **`pgdata:/var/lib/postgresql/data`** para persistencia.
- **api** (FastAPI + Uvicorn): REST + encolado Celery.
- **rabbitmq**: cola de mensajes (AMQP) con UI de administraci√≥n.
- **worker** (Celery): ejecuta tareas de la cola.
- **airflow-webserver / airflow-scheduler / airflow-init**: orquestaci√≥n y UI.

> Importante: el script `db-init/01-init.sql` solo corre cuando el datadir est√° vac√≠o. Una vez inicializada la DB, ya no se vuelve a ejecutar.

---

## üìö Modelo de datos (simple)

- **items**
  - `id` (PK)
  - `name` (UNIQUE) ‚Äî idempotencia por nombre
  - `price` (NUMERIC)
- **item_prices** (hist√≥rico append-only)
  - `id` (PK), `item_name`, `price`, `run_ts` (timestamp)

---

## üîå Endpoints (demo r√°pida)

```bash
# Health
curl http://localhost:8000/health

# Crear item
curl -X POST http://localhost:8000/items \
  -H "Content-Type: application/json" \
  -d "{\"name\":\"demo\",\"price\":100}"

# Procesar item (reemplaza ID por el que devolvi√≥ la creaci√≥n)
curl -X POST http://localhost:8000/items/ID/process

# Hist√≥rico por nombre
curl http://localhost:8000/items/product_0/history

# √öltimos precios por item
curl http://localhost:8000/items/latest
```

> En Windows/PowerShell, si un `curl` JSON falla por comillas, us√° esta variante:
>
> ```powershell
> curl.exe --% -X POST http://localhost:8000/items -H "Content-Type: application/json" -d "{\"name\":\"demo\",\"price\":100}"
> ```

---

## üõ†Ô∏è DAGs de Airflow

### `daily_prices_etl`

- Genera CSV con precios aleatorios, transforma con pandas.
- Carga con **UPSERT** (`ON CONFLICT (name) DO UPDATE`) en `items`.
- Inserta una fila por item en `item_prices` con el `run_ts` de la corrida.
- **Idempotente**: no duplica nombres en `items`; solo actualiza `price`.

### `cleanup_item_prices`

- Retenci√≥n: borra filas de `item_prices` con `run_ts` anterior a `N` d√≠as (ej. 30).
- Configurable por variables de entorno (`RETENTION_DAYS`, `DRY_RUN`).

### `backup_pg` (opcional)

- `pg_dump` diario de `tcsdb` hacia `/opt/airflow/backups` (montar volumen si quer√©s persistir en host).

#### Comandos √∫tiles

```bash
# Disparar ETL manual dos veces (ver idempotencia e hist√≥rico)
docker compose exec airflow-webserver airflow dags trigger daily_prices_etl
docker compose exec airflow-webserver airflow dags trigger daily_prices_etl

# Ver logs del scheduler
docker compose logs --tail=120 airflow-scheduler
```

---

## üíæ Persistencia y Backups

### Persistencia

Asegurate de tener en `docker-compose.yml`:

```yaml
services:
  db:
    volumes:
      - ./db-init/01-init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - pgdata:/var/lib/postgresql/data   # <- volumen persistente

volumes:
  pgdata:
```

### Backups manuales

```bash
# Dump
docker compose exec db pg_dump -U tcs -d tcsdb > backup_tcsdb.sql

# Restore
cat backup_tcsdb.sql | docker compose exec -T db psql -U tcs -d tcsdb
```

---

## ‚úÖ Calidad y CI

- **pre-commit**: `ruff`, `ruff-format`, `black` sobre el repo.
- **GitHub Actions** (sugerido): lint + build de imagen `api` en cada push/PR.

```bash
pre-commit run --all-files
```

---

## üß™ Verificaciones r√°pidas

```bash
# API viva
curl http://localhost:8000/health

# DB sin duplicados por nombre (items)
docker compose exec db psql -U tcs -d tcsdb -c "SELECT COUNT(*) total, COUNT(DISTINCT name) distintos FROM items;"

# Hist√≥rico poblado
docker compose exec db psql -U tcs -d tcsdb -c "SELECT item_name, COUNT(*) FROM item_prices GROUP BY item_name ORDER BY item_name;"
```

---

## ü©∫ Troubleshooting (Windows / PowerShell / Airflow)

- **`curl` JSON falla** ‚Üí usar `curl.exe --% ...` o escapar comillas como en ejemplos.
- **`airflow-webserver` no corre** ‚Üí revisar logs (`docker compose logs airflow-webserver`), errores de import/sintaxis en DAGs, y reiniciar scheduler/webserver.
- **`UNIQUE VIOLATION` en ETL** ‚Üí confirmar que el `INSERT` usa `ON CONFLICT (name) DO UPDATE`.
- **API 404 en endpoint nuevo** ‚Üí reconstruir imagen (`docker compose up -d --build api`), la app dentro del contenedor debe recargar c√≥digo.

---

## üßë‚Äçüíª Extensiones sugeridas (para seguir)

- **Streamlit** como UI de carga (formulario y upload CSV) llamando a la API.
- **Endpoint `/upload-csv`** para carga masiva con validaciones.
- **Alembic** para versionado de esquema.
- **Despliegue en VM** (Docker Compose + proxy Nginx + HTTPS) o DB gestionada (RDS/Cloud SQL/Supabase).

---

## üìå Bullets

- Backend **FastAPI** con endpoints de negocio y carga masiva.
- **ETL diario en Airflow** con **UPSERT** e hist√≥rico `item_prices`.
- **RabbitMQ + Celery** para procesamiento as√≠ncrono desde la API.
- **Docker Compose** con **PostgreSQL persistente** y healthchecks.
- Calidad con **pre-commit** (ruff/black) y **CI** (GitHub Actions).
- (Opcional) UI **Streamlit** para carga manual y por CSV.
